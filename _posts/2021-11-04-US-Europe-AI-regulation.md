---
layout: post
title: "U.S. and European AI Regulation"
date: 2021-11-04 14:15:44 -0000
---

__Note__: This is a question I was given on a test task after applying to a fellowship on AI policy (not the same occupational entity as in [Delaying Transformative AI](https://tmartin2.github.io/blog/2021/09/16/Delaying-Transformative-AI.html)). I spent roughly an hour writing my response. Here is the question:

_Would tighter regulation of AI technologies in the U.S. and Europe overall substantially reduce, substantially increase, or have little net effect on catastrophic risks? What considerations are important here? Note that your reasoning here is more important than getting a "right answer"._

Here is my answer. Later on, I describe criticisms of my responses, and detail some considerations that I missed in my response.

## My Response

From a short-termist perspective, I take AI technologies to include current and planned usages of AI, such as in facial recognition, autonomous weapons, recommender systems, surveillance, etc… and, from a more long-termist perspective, I take AI technologies to include transformative AI (as defined by OpenPhil: AI that engenders changes that are equal to or exceed those engendered by either the agricultural or industrial revolutions).

To resolve this question, the following considerations are crucial:
- How much catastrophic risk does short-term and long-term AI pose?
- What forms of AI regulation do / will the U.S. and Europe impose by default (regulation as it stands), and how efficacious is this regulation be in terms of reducing catastrophic risk incurred by AI technologies?
- If the default does not sufficiently reduce risk, then what is the minimal tightening of regulation necessary to make the risks marginal?
- Plausibly, how far would U.S. and European political institutions go in tightening AI technology regulation, if something dire were to become known or expected.
- What is the likelihood that a threat from AI emerges beyond the scope the of U.S. or European control?
- What is the likelihood that any U.S. or European regulation of AI technology reduces the answer to the previous inquiry?

In general, these considerations flow in the following manner: how could AI engender poor outcomes?chance of these outcomes? new chance after more regulation? instrumental: how far could regulation even go? okay, but what if threat emerges outside U.S. or Europe? how does U.S. / European policy affect extranational AI risks?

Since there are so many variables that factor into these considerations, I am not going to estimate the reductions in AI risk conferred by tighter regulation in the U.S. and Europe as a percentage or range of percentages, and will instead stick with the categories [‘substantial reduction’, ‘some reduction, but still not entirely safe’, ‘no net reduction’, ‘mild increased risk’, ‘substantial risk increase’].

In my experience exploring AI risk, it appears that present-day uses of AI pose some form of catastrophic risk primarily by increasing the risk of stable totalitarian systems (think constant biometric monitoring, behavioral cataloging, and upending systems of dissent) and by increasing the risk of nuclear war (think autonomous weapons and launch systems). With human coordination, contemporaneous AI technology can likely be successfully directed towards these malevolent ends. This will only be easier as time passes in the short-term (e.g. 20 years from now). As for long-term risk from AI, the tails of possibility extend far outwards, and uncertainty abounds; presently, all that can be entertained are various catastrophic scenarios, but these are likely only a subset of what’s possible. Therefore, I think that transformative AI poses a significantly stronger risk for catastrophe than short-term AI technologies.

The U.S. and European political systems of today are continually outpaced by technological developments across the board, and the same is true for applications of present-day AI technology. Courtrooms, police forces, tech-companies, etc… escape with little oversight on what uses of machine learning are morally or legally acceptable. Of course, this may change with time, as more technologically informed citizenry enters the political domain, as more egregious uses of present-day AI technologies become mainstream news, and as machine learning technologies becomes more programmatically accessible. While this ‘default condition’ might teeter on the edge of being able to substantially reduce catastrophic risk posed short-term AI tech., I belief it is insufficient to reduce long-term AI threats.

Tighter regulation might help with both veins (short-term / long-term) of risk reduction, but by how much remains very uncertain. I’d estimate that, in the case much stricter regulation of AI tech. was imposed sometime in the next 10-20 years, short-term risks would experience the greatest quantity of risk reduction, as most regulation of AI tech. would pertain to application over research. This distinction is extremely important, as the short-term applications of AI technology, which are the most publicly visible and contribute to the brunt of societal discussion on AI, pose significantly less risk than AI research, which is less visible but much more likely to engender catastrophe. Should regulation touch upon AI research by tech. companies and universities, I think that the long-term risks posed by AI would be somewhat reduced.

I would strongly bet that, if transformative AI is created, then its inception would occur in one of: Five Eyes or Europe or China. So, what is the likelihood that it occurs in this group minus the U.S. and Europe, and by how much would tighter U.S. or European AI technological regulations affect the risk of catastrophe in these places? I think that tighter regulation from the U.S. and Europe would result in an equivalent quantity of risk reduction in the rest of the Five Eyes, but would not affect China. My intuition from news + Metaculus, is that China’s research of AI and uses of present-day AI operate parallel to the rest of the nations / groups mentioned. This means that tighter U.S. and European regulation of AI tech. would not have much effect the AI risk stemming from China, and may even increase the catastrophic risk by incentivizing China to take advantage of the U.S. and Europe’s ‘stagnation’, i.e. to get the top place in an AI arms race.

Altogether, I’d estimate that:
Most catastrophic risk posed by AI comes from transformative AI in the long-term, and not from the short-term applications of AI in society, which are more well-known than the various research enterprises occurring in academia and ‘behind closed doors’.

A tightening of AI regulation pertaining to applications of present-day AI would result in ‘some reduction, but still not entirely safe’ for short-term AI threats, ‘no net reduction’ for long-term risks, and ‘mild increased risk’ for short-term threats by entities outside the U.S. and Europe. On net: ‘no net reduction’.

A tightening of AI regulation pertaining to research of present-day (which would be less visible to mainstream audiences) AI would result in ‘some reduction, but still not entirely safe’ for short-term AI threats, ‘some reduction, but still not entirely safe’ for long-term risks, and ‘no net reduction’ for short-term threats by entities outside the U.S. and Europe. On net: ‘some reduction, but still not entirely safe’

A tightening of AI regulation pertaining to research AND applications of present-day AI would result in ‘substantial reduction’ for short-term AI threats, ‘some reduction, but still not entirely safe’ for long-term risks, and ‘mild increased risk’ for short-term threats by entities outside the U.S. and Europe. On net: ‘some reduction, but still not entirely safe’

From this exercise, it appears now to me that a tightening of regulation would ofter some reduction in the catastrophic risk posed by AI, but not to an extent that would be entirely safe. For society to be entirely safe, I think there would have to be a fundamental shift in governmental and researcher incentives, and/or for AI ethics / safety to be adopted by these actors.

<!-- ## Criticisms and Additional Considerations -->
